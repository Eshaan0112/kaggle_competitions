{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c41529",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-19T21:10:20.441553Z",
     "iopub.status.busy": "2024-11-19T21:10:20.441154Z",
     "iopub.status.idle": "2024-11-19T21:10:23.814609Z",
     "shell.execute_reply": "2024-11-19T21:10:23.813549Z"
    },
    "papermill": {
     "duration": 3.382126,
     "end_time": "2024-11-19T21:10:23.816889",
     "exception": false,
     "start_time": "2024-11-19T21:10:20.434763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\n",
      "/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\n",
      "/kaggle/input/house-prices-advanced-regression-techniques/train.csv\n",
      "/kaggle/input/house-prices-advanced-regression-techniques/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# All necessary imports\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Scikit learn\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9082b3c2",
   "metadata": {
    "papermill": {
     "duration": 0.003955,
     "end_time": "2024-11-19T21:10:23.825279",
     "exception": false,
     "start_time": "2024-11-19T21:10:23.821324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c923105",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T21:10:23.835270Z",
     "iopub.status.busy": "2024-11-19T21:10:23.834724Z",
     "iopub.status.idle": "2024-11-19T21:10:23.878977Z",
     "shell.execute_reply": "2024-11-19T21:10:23.877691Z"
    },
    "papermill": {
     "duration": 0.052245,
     "end_time": "2024-11-19T21:10:23.881499",
     "exception": false,
     "start_time": "2024-11-19T21:10:23.829254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full training data shape: (1460, 81)\n"
     ]
    }
   ],
   "source": [
    "# Load full training data as a dataframe\n",
    "input_train_file = \"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\"\n",
    "fulltrain_df = pd.read_csv(input_train_file)\n",
    "print(f'Full training data shape: {fulltrain_df.shape}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae68fe3",
   "metadata": {
    "papermill": {
     "duration": 0.00407,
     "end_time": "2024-11-19T21:10:23.889900",
     "exception": false,
     "start_time": "2024-11-19T21:10:23.885830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08421fde",
   "metadata": {
    "papermill": {
     "duration": 0.003885,
     "end_time": "2024-11-19T21:10:23.897955",
     "exception": false,
     "start_time": "2024-11-19T21:10:23.894070",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f50d8a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T21:10:23.908131Z",
     "iopub.status.busy": "2024-11-19T21:10:23.907728Z",
     "iopub.status.idle": "2024-11-19T21:10:23.923126Z",
     "shell.execute_reply": "2024-11-19T21:10:23.921698Z"
    },
    "papermill": {
     "duration": 0.023376,
     "end_time": "2024-11-19T21:10:23.925527",
     "exception": false,
     "start_time": "2024-11-19T21:10:23.902151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    This function removes columns from training that have >80% NaN values.\n",
    "    You can play with the percentage of NaN values to see what works best.\n",
    "    \"\"\"\n",
    "    samples = df.shape[0] # number of samples in the data\n",
    "    cols_to_be_dropped = [] # columns with majority null values\n",
    "    \n",
    "    # Remove columns from training that have >80% missing columns\n",
    "    for c in df.columns.to_list():\n",
    "        num_nulls = df[c].isnull().sum() # calculate the sum of null values\n",
    "        percent_nulls = num_nulls/samples\n",
    "        if percent_nulls > 0.8:\n",
    "            cols_to_be_dropped.append(c)\n",
    "    print(f'Columns that have more than 80% NaN values:{cols_to_be_dropped}')\n",
    "    df = df.drop(cols_to_be_dropped, axis=1)\n",
    "    return df\n",
    "    \n",
    "def handle_skewness(skew, df):\n",
    "    \"\"\"\n",
    "    This function handles skewness in data and performs log transformation on features only.\n",
    "    ** Not being used in preprocessing as it didn't help in accuracy\n",
    "    \"\"\"\n",
    "    # Log transform for features with skewness > 2\n",
    "    for col in df.columns.to_list():\n",
    "        if skew[col] > 2:  # For positively skewed\n",
    "            df[col] = np.log1p(df[col])  # log1p handles 0 and positive values (log(1+x))\n",
    "        elif skew[col] < -2:  # For negatively skewed\n",
    "            min_val = abs(df[col].min()) + 1  # shift the column to make all values positive\n",
    "            df[col] = np.log1p(df[col] + min_val)  # log1p after shifting \n",
    "    return df\n",
    "\n",
    "def visualize_and_remove_outliers(df, Y):\n",
    "    \"\"\"\n",
    "    This function plots the scatterplots for every feature against the label.\n",
    "    I used it to visually filter out outliers from some of the features.\n",
    "    ** Might be buggy but works as a weak checker\n",
    "    \"\"\"\n",
    "    numerical_cols = df.select_dtypes(exclude=['object']).columns\n",
    "    n_cols = 6\n",
    "    n_features = len(numerical_cols)  # total number of features\n",
    "    n_rows = (n_features // n_cols)  # compute number of rows (ceiling division)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(25,20), constrained_layout=True)\n",
    "    axes = axes.flatten() # flatten to 1D\n",
    "    print(n_rows, n_cols, n_features)\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        ax = axes[i]\n",
    "        clean_data = pd.concat([df[col], Y], axis=1).dropna()\n",
    "        ax.scatter(clean_data[col], clean_data[Y.name], alpha=0.7, edgecolor='k')\n",
    "        ax.set_title(f\"{col} vs Y\", fontsize=10)\n",
    "        ax.set_xlabel(col, fontsize=8)\n",
    "        ax.set_ylabel(\"Y\", fontsize=8)\n",
    "        ax.tick_params(axis='both', labelsize=8)\n",
    "\n",
    "    plt.suptitle(\"Scatterplots of Features vs Target\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "def remove_outliers(df):\n",
    "    \"\"\"\n",
    "    This function removes the visually inspected outliers.\n",
    "    The list of the outliers is given in the cell below.\n",
    "    \"\"\"\n",
    "    conditions = ( \n",
    "        (df['BsmtFinSF2'] <= 1200) &  \n",
    "        (df['BsmtFinSF1'] <= 3000) &  \n",
    "        (df['MasVnrArea'] <= 1200) &  \n",
    "        (df['EnclosedPorch'] <= 400) &\n",
    "        (df['GrLivArea']<=4000)\n",
    "    )\n",
    "\n",
    "    filtered_df = df[conditions].reset_index(drop=True) \n",
    "    return filtered_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbc75707",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T21:10:23.936160Z",
     "iopub.status.busy": "2024-11-19T21:10:23.935761Z",
     "iopub.status.idle": "2024-11-19T21:10:23.943692Z",
     "shell.execute_reply": "2024-11-19T21:10:23.942651Z"
    },
    "papermill": {
     "duration": 0.015853,
     "end_time": "2024-11-19T21:10:23.945873",
     "exception": false,
     "start_time": "2024-11-19T21:10:23.930020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# On visual inspection here are the outliers:\n",
    "'''\n",
    "Only numerical features:\n",
    "1. BsmtFinSF2 > 1200\n",
    "2. BsmtFinSF1 > 3000\n",
    "3. MasVnrArea > 1200\n",
    "4. EnclosedPorch > 400\n",
    "5. GrLivArea > 4000\n",
    "'''   \n",
    "\n",
    "def preprocess(fulltrain_df, training):\n",
    "    \"\"\"\n",
    "    This function pre-processes the data by encoding categoricals and imputing NaN values using target encoding\n",
    "    \"\"\"\n",
    "    # visualize_outliers(fulltrain_df) # visulaize outliers for each feature - helper function: not to be used in general training\n",
    "    # visualize_and_remove_outliers(fulltrain_df, Y)\n",
    "\n",
    "    # Remove outliers\n",
    "    if training:\n",
    "        fulltrain_df = remove_outliers(fulltrain_df)\n",
    "    \n",
    "    # Drop columns with >80% null values\n",
    "    fulltrain_df = clean_data(fulltrain_df)\n",
    "\n",
    "    # fulltrain_df.info() # info of all feature columns\n",
    "    categoricals = fulltrain_df.select_dtypes(include='object') # object = categoricals\n",
    "    categorical_column_names = categoricals.columns # list of categoricals\n",
    "    # print(categoricals.nunique()) # unique categories for each category\n",
    "    columns_with_nan = fulltrain_df.columns[fulltrain_df.isna().any()].tolist() # NaN columns to be imputed\n",
    "\n",
    "    # Encode categoricals\n",
    "    le = LabelEncoder()\n",
    "    for col in categorical_column_names.to_list():\n",
    "        fulltrain_df[col] = le.fit_transform(fulltrain_df[col]) # inplace encode categorical columns\n",
    "    \n",
    "    # print(fulltrain_df[categorical_column_names.to_list()].head(1)) # simple check to see encodings\n",
    "    # if above statement shows an empty dataframe, it's possible the script was run before and cateforicals have already been encoded\n",
    "    \n",
    "    # Impute missing values using mean of all other entries (target encoding) - here the categoricals are encoded so the mean approach works\n",
    "    for column in columns_with_nan:\n",
    "        mean_value = fulltrain_df[column].mean()  \n",
    "        fulltrain_df[column].fillna(mean_value, inplace=True)  \n",
    "\n",
    "    # Check for highly skewed features and transform them if skewness is > abs(2) - not used in training because it didn't help in accuracy\n",
    "    # skew = fulltrain_df.skew()\n",
    "    # print(fulltrain_df['SalePrice'])\n",
    "    # fulltrain_df = handle_skewness(skew, fulltrain_df)\n",
    "        \n",
    "    return fulltrain_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25bc680",
   "metadata": {
    "papermill": {
     "duration": 0.004962,
     "end_time": "2024-11-19T21:10:23.955095",
     "exception": false,
     "start_time": "2024-11-19T21:10:23.950133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Apply log transformation and split training/testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48c74ba7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T21:10:23.965576Z",
     "iopub.status.busy": "2024-11-19T21:10:23.964653Z",
     "iopub.status.idle": "2024-11-19T21:10:24.047691Z",
     "shell.execute_reply": "2024-11-19T21:10:24.046643Z"
    },
    "papermill": {
     "duration": 0.091048,
     "end_time": "2024-11-19T21:10:24.050279",
     "exception": false,
     "start_time": "2024-11-19T21:10:23.959231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that have more than 80% NaN values:['Alley', 'PoolQC', 'Fence', 'MiscFeature']\n",
      "Training data shape: (1155, 75)\n",
      "Testing data shape: (289, 75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/1806926364.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  fulltrain_df[column].fillna(mean_value, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "fulltrain_df = preprocess(fulltrain_df, training = True)\n",
    "\n",
    "# Split labels and features\n",
    "Y = fulltrain_df['SalePrice']\n",
    "fulltrain_df.drop(['SalePrice'], inplace=True, axis=1)\n",
    "X_ids = fulltrain_df['Id'] # save IDs to put in later to match columns of testing data\n",
    "X = fulltrain_df.drop(columns=['Id'],axis=1) # we don't need Id column for training\n",
    "\n",
    "Y = np.log1p(Y) # log transformation to convert rightly skewed labels to normal distributions - helps in accuracy\n",
    "# Can use this to visualize the rightly skewed distribution of Y emphasizing the need for log transormation\n",
    "# plt.hist(Y, bins=30, edgecolor='k', alpha=0.7)\n",
    "# plt.title(\"Histogram of Y\")\n",
    "# plt.xlabel(\"Y values\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.show()\n",
    "\n",
    "# Split training/testing for training data before submission\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "print(f'Training data shape: {X_train.shape}')\n",
    "print(f'Testing data shape: {X_test.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5496e7f",
   "metadata": {
    "papermill": {
     "duration": 0.004832,
     "end_time": "2024-11-19T21:10:24.060021",
     "exception": false,
     "start_time": "2024-11-19T21:10:24.055189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c4a4f9",
   "metadata": {
    "papermill": {
     "duration": 0.004343,
     "end_time": "2024-11-19T21:10:24.069072",
     "exception": false,
     "start_time": "2024-11-19T21:10:24.064729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Without tuned hyperparameters on train/test split\n",
    "### just used as a check. The next cell trains the model on the entire training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eb28a9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T21:10:24.080020Z",
     "iopub.status.busy": "2024-11-19T21:10:24.079564Z",
     "iopub.status.idle": "2024-11-19T21:10:25.951140Z",
     "shell.execute_reply": "2024-11-19T21:10:25.949860Z"
    },
    "papermill": {
     "duration": 1.88001,
     "end_time": "2024-11-19T21:10:25.953609",
     "exception": false,
     "start_time": "2024-11-19T21:10:24.073599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 test score (before tuning hyperparameters): 0.9007953416565198\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred = rf_model.predict(X_test)\n",
    "# We don't need to \"untransform\" our predictions here because Y was divided into training and testing sets, so the predictions will be log transformed and so will the testing labels\n",
    "print(f'r2 test score (before tuning hyperparameters): {r2_score(y_test,y_pred)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac13326",
   "metadata": {
    "papermill": {
     "duration": 0.005462,
     "end_time": "2024-11-19T21:10:25.965087",
     "exception": false,
     "start_time": "2024-11-19T21:10:25.959625",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## With tuned hyperparameters on full train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eb20d83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T21:10:25.979112Z",
     "iopub.status.busy": "2024-11-19T21:10:25.978733Z",
     "iopub.status.idle": "2024-11-19T21:11:06.407135Z",
     "shell.execute_reply": "2024-11-19T21:11:06.405767Z"
    },
    "papermill": {
     "duration": 40.437098,
     "end_time": "2024-11-19T21:11:06.409483",
     "exception": false,
     "start_time": "2024-11-19T21:10:25.972385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'n_estimators': 230, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "# Randomized search for hyperparameters\n",
    "rf_model = RandomForestRegressor() # redefine model object\n",
    "random_grid = {'bootstrap': [True, False], # domain to search \n",
    "               'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'min_samples_leaf': [1, 2, 4],\n",
    "               'min_samples_split': [2, 5, 10],\n",
    "               'n_estimators': [130, 180, 230]}\n",
    "\n",
    "# Search across (n_iters * cv)  different combinations, and use all available cores\n",
    "rf_model = RandomizedSearchCV(estimator = rf_model, param_distributions = random_grid, n_iter = 20, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_model.fit(X, Y)\n",
    "print(f'Best parameters:')\n",
    "print(rf_model.best_params_)\n",
    "\n",
    "# Will eventually implement Bayesian Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af5aafd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T21:11:06.423728Z",
     "iopub.status.busy": "2024-11-19T21:11:06.423262Z",
     "iopub.status.idle": "2024-11-19T21:11:07.574993Z",
     "shell.execute_reply": "2024-11-19T21:11:07.573907Z"
    },
    "papermill": {
     "duration": 1.161726,
     "end_time": "2024-11-19T21:11:07.577307",
     "exception": false,
     "start_time": "2024-11-19T21:11:06.415581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 test score (after hyperparameter tuning): 0.9062164295873577\n"
     ]
    }
   ],
   "source": [
    "# Simple check on train/test split to see advantage of hyperparameter tuning\n",
    "rf_check = RandomForestRegressor(**rf_model.best_params_)\n",
    "rf_check.fit(X_train, y_train) # remember Y is log transformed i.e Y = log(Y+1)\n",
    "y_pred = rf_check.predict(X_test)\n",
    "print(f'r2 test score (after hyperparameter tuning): {r2_score(y_test, y_pred)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11df1ff",
   "metadata": {
    "papermill": {
     "duration": 0.005689,
     "end_time": "2024-11-19T21:11:07.589035",
     "exception": false,
     "start_time": "2024-11-19T21:11:07.583346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict for test data and prepare submission file under /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c98e683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T21:11:07.602505Z",
     "iopub.status.busy": "2024-11-19T21:11:07.602143Z",
     "iopub.status.idle": "2024-11-19T21:11:07.779007Z",
     "shell.execute_reply": "2024-11-19T21:11:07.777700Z"
    },
    "papermill": {
     "duration": 0.188083,
     "end_time": "2024-11-19T21:11:07.783078",
     "exception": false,
     "start_time": "2024-11-19T21:11:07.594995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that have more than 80% NaN values:[]\n",
      "Check /kaggle/working directory for 'submission' submission_13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/1806926364.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  fulltrain_df[column].fillna(mean_value, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Submission data    \n",
    "submission_name = 'submission_13' # change as per submission number\n",
    "input_test_file = \"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\"\n",
    "\n",
    "# Read and preprocess testing data\n",
    "submission_df = pd.read_csv(input_test_file)\n",
    "X['Id'] = X_ids # add already saved Ids to simply match all columns between training and testing data - hacky\n",
    "submission_df = submission_df[X.columns] # there are some columns that are removed as part of pre-processing in training and maintaining consistency between training and testing sets is needed otherwise an error will be thrown\n",
    "submission_df = preprocess(submission_df, training = False)\n",
    "\n",
    "\n",
    "# Predict using trained model\n",
    "ids = submission_df.pop('Id') # save IDs\n",
    "y_pred_submission = rf_model.predict(submission_df)\n",
    "\n",
    "# Prepare csv submission\n",
    "output = pd.DataFrame({'Id': ids,\n",
    "                       'SalePrice': np.expm1(y_pred_submission)}) # converting predicted labels back from log transformation\n",
    "output.to_csv(f'/kaggle/working/{submission_name}.csv', index=False) \n",
    "print(f\"Check /kaggle/working directory for 'submission' {submission_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a9886f",
   "metadata": {
    "papermill": {
     "duration": 0.006092,
     "end_time": "2024-11-19T21:11:07.795802",
     "exception": false,
     "start_time": "2024-11-19T21:11:07.789710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d78eb9",
   "metadata": {
    "papermill": {
     "duration": 0.005842,
     "end_time": "2024-11-19T21:11:07.807818",
     "exception": false,
     "start_time": "2024-11-19T21:11:07.801976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 868283,
     "sourceId": 5407,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 52.856529,
   "end_time": "2024-11-19T21:11:10.434120",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-19T21:10:17.577591",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
